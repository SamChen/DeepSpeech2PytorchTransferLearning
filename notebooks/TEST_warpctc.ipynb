{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from warpctc_pytorch import CTCLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import log_softmax, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand([3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8030, 0.9967, 0.2067, 0.8917],\n",
       "        [0.4976, 0.4992, 0.9585, 0.0284],\n",
       "        [0.3995, 0.4733, 0.6297, 0.5109]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2593, 0.3147, 0.1428, 0.2833],\n",
       "        [0.2374, 0.2378, 0.3764, 0.1485],\n",
       "        [0.2246, 0.2418, 0.2827, 0.2510]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.softmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Linear(4, 5)\n",
    "model.train()\n",
    "print(model.training)\n",
    "model.eval()\n",
    "print(model.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.9217e+01,  2.6168e-02, -1.7620e+01,  ..., -2.1196e-01,\n",
       "           2.7001e-02,  2.6985e-02],\n",
       "         [-1.4188e+02,  2.7052e-02, -2.1667e+01,  ..., -2.6427e+01,\n",
       "           2.6948e-02,  2.6962e-02],\n",
       "         [-3.0149e+01,  2.7156e-02, -2.2205e-01,  ...,  2.6966e-02,\n",
       "           2.6981e-02,  2.7041e-02],\n",
       "         ...,\n",
       "         [-4.7550e-01,  2.7008e-02,  2.6979e-02,  ...,  2.7016e-02,\n",
       "           2.6988e-02,  2.6973e-02],\n",
       "         [-5.7200e-01,  2.7046e-02,  2.6949e-02,  ...,  2.7166e-02,\n",
       "           2.6939e-02,  2.7189e-02],\n",
       "         [-3.0327e-01,  2.6984e-02,  2.6975e-02,  ...,  2.6920e-02,\n",
       "           2.6957e-02,  2.7077e-02]],\n",
       "\n",
       "        [[-8.9617e+02,  1.1260e-03, -2.7600e+02,  ..., -4.4240e+00,\n",
       "           2.6923e-02,  2.7049e-02],\n",
       "         [-2.1263e+03,  2.7022e-02, -3.2548e+02,  ..., -3.7400e+02,\n",
       "           2.7111e-02,  2.7017e-02],\n",
       "         [-7.4793e+02,  2.7092e-02, -5.9343e+00,  ...,  2.6944e-02,\n",
       "           2.7086e-02,  2.7055e-02],\n",
       "         ...,\n",
       "         [-3.5144e-01,  2.7137e-02,  2.6914e-02,  ...,  2.7004e-02,\n",
       "           2.7100e-02,  2.6989e-02],\n",
       "         [-4.2709e-01,  2.7043e-02,  2.6973e-02,  ...,  2.6955e-02,\n",
       "           2.7110e-02,  2.7141e-02],\n",
       "         [-2.3165e-01,  2.7026e-02,  2.6976e-02,  ...,  2.7114e-02,\n",
       "           2.6998e-02,  2.7001e-02]],\n",
       "\n",
       "        [[-1.4890e+04, -5.0360e-01, -4.3253e+03,  ..., -8.2303e+01,\n",
       "           2.7094e-02,  2.7129e-02],\n",
       "         [-3.0463e+04,  2.6983e-02, -4.8220e+03,  ..., -5.2381e+03,\n",
       "           2.6927e-02,  2.6927e-02],\n",
       "         [-1.8583e+04,  2.6955e-02, -1.6684e+02,  ...,  2.7001e-02,\n",
       "           2.7173e-02,  2.7126e-02],\n",
       "         ...,\n",
       "         [-3.2019e-01,  2.6958e-02,  2.6951e-02,  ...,  2.7017e-02,\n",
       "           2.6944e-02,  2.7033e-02],\n",
       "         [-3.7401e-01,  2.7069e-02,  2.6959e-02,  ...,  2.7158e-02,\n",
       "           2.7079e-02,  2.6971e-02],\n",
       "         [-2.2271e-01,  2.6952e-02,  2.7062e-02,  ...,  2.7147e-02,\n",
       "           2.6984e-02,  2.6970e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[       -inf,        -inf,  2.6931e-02,  ...,  2.7087e-02,\n",
       "           2.7140e-02,  2.6916e-02],\n",
       "         [       -inf,  2.7183e-02,        -inf,  ...,        -inf,\n",
       "           2.7171e-02,  2.6951e-02],\n",
       "         [       -inf,        -inf,        -inf,  ...,  2.6934e-02,\n",
       "           2.7062e-02,  2.6929e-02],\n",
       "         ...,\n",
       "         [-3.0907e-01,  2.7104e-02,  2.6920e-02,  ...,  1.5781e-02,\n",
       "           2.6949e-02,  2.6983e-02],\n",
       "         [-3.4782e-01,  2.7042e-02, -7.7220e-03,  ...,  2.7153e-02,\n",
       "           2.6993e-02,  2.7005e-02],\n",
       "         [-2.2085e-01, -2.7174e-02,  2.6954e-02,  ..., -1.1497e-01,\n",
       "           2.6932e-02,  2.7048e-02]],\n",
       "\n",
       "        [[       -inf,        -inf,  2.6957e-02,  ...,  2.7034e-02,\n",
       "           2.7043e-02,  2.6977e-02],\n",
       "         [       -inf,  2.6989e-02,        -inf,  ...,        -inf,\n",
       "           2.6958e-02,  2.7022e-02],\n",
       "         [       -inf,        -inf,        -inf,  ...,  2.6966e-02,\n",
       "           2.6980e-02,  2.6994e-02],\n",
       "         ...,\n",
       "         [-3.0848e-01,  2.6985e-02,  2.7191e-02,  ...,  1.9929e-02,\n",
       "           2.6980e-02,  2.6965e-02],\n",
       "         [-3.4792e-01,  2.7030e-02,  5.9008e-04,  ...,  2.7085e-02,\n",
       "           2.7056e-02,  2.6937e-02],\n",
       "         [-2.2138e-01, -4.6803e-02,  2.7044e-02,  ..., -1.1113e-01,\n",
       "           2.7146e-02,  2.6965e-02]],\n",
       "\n",
       "        [[       -inf,        -inf,  2.7123e-02,  ...,  2.7110e-02,\n",
       "           2.6911e-02,  2.7020e-02],\n",
       "         [       -inf,  2.7102e-02,        -inf,  ...,        -inf,\n",
       "           2.6984e-02,  2.6986e-02],\n",
       "         [       -inf,        -inf,        -inf,  ...,  2.7036e-02,\n",
       "           2.6979e-02,  2.7021e-02],\n",
       "         ...,\n",
       "         [-3.0879e-01,  2.6957e-02,  2.6935e-02,  ...,  2.2808e-02,\n",
       "           2.7003e-02,  2.6980e-02],\n",
       "         [-3.4852e-01,  2.7036e-02,  7.4996e-03,  ...,  2.6920e-02,\n",
       "           2.6923e-02,  2.6945e-02],\n",
       "         [-2.2144e-01, -6.8203e-02,  2.6997e-02,  ..., -9.9521e-02,\n",
       "           2.6985e-02,  2.7048e-02]]], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costfunc = CTCLoss()\n",
    "\n",
    "preds = torch.rand(100, 128, 37)\n",
    "# input predict should be softmaxed\n",
    "preds = torch.softmax(preds, dim=0, dtype=torch.float32)\n",
    "preds = preds.to(\"cuda\")\n",
    "preds = preds.requires_grad_(True)\n",
    "\n",
    "label_length = torch.from_numpy(np.random.randint(20, 80, 128, dtype=np.int32))\n",
    "s = label_length.sum()\n",
    "s = int(s)\n",
    "\n",
    "labels = torch.from_numpy(np.random.randint(1, 36, (s,), dtype=np.int32))\n",
    "\n",
    "# length of input. Since the input is a tensor, the input size of all_seq in one batch should be the same \n",
    "preds_size = torch.tensor([128 for i in range(128)], dtype=torch.int32)\n",
    "\n",
    "loss = costfunc(preds, labels, preds_size, label_length)\n",
    "loss.backward()\n",
    "preds.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
