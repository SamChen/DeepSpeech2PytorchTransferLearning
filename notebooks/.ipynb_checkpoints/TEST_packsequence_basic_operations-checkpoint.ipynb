{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_sequence, PackedSequence\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils.custom_attentions import NTMAtten_Layer, DotAtten_Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def packedsequences_merge(packed_data_list, num_layers=2):\n",
    "    packed_data_list = [i.unsqueeze(dim=-2) for i in packed_data_list]\n",
    "    packed_data_list= torch.cat(packed_data_list, dim=-2)\n",
    "    packed_data = PackedSequence(packed_data_list, batchs) # TxB,num_Data,size_F\n",
    "    \n",
    "    return packed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interatie_packedsequences(sp: PackedSequence):\n",
    "    data, batchs, _, _ = sp\n",
    "    st_index = 0\n",
    "    for indexf, b in enumerate(batchs):\n",
    "        # return Tx1, num_Data, size_F\n",
    "        yield data[st_index: st_index+b, :, :], indexf \n",
    "        st_index += b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "# batch_size=5, seq_length=6, feature_size=3\n",
    "data = [torch.rand(5, 6, 3).requires_grad_()  for i in range(num_layers)]\n",
    "\n",
    "packed_data_list = []\n",
    "for i in data:\n",
    "    temp, batchs, *_ = pack_sequence(i)\n",
    "    packed_data_list.append(temp)\n",
    "\n",
    "    \n",
    "packed_data, data = generate_test_data(packed_data_list, num_layers)\n",
    "# check the interator works properately.\n",
    "test_att = NTMAtten_Layer(embed_dim=3, window_size=1)\n",
    "for frame, t in interatie_packedsequences(packed_data):\n",
    "    for i in range(num_layers):\n",
    "        assert torch.equal(frame[:, i,:],data[i][:, t, :]), \"{}\\n{} \\n==============\\n {}\".format(i,frame[:, i,:],data[i][:, t, :])\n",
    "        \n",
    "    output_hat, weights = test_att(torch.rand(5, 1, 3), frame, frame)\n",
    "    output = torch.ones(5, 1, 3)\n",
    "    loss = torch.abs(output - output_hat).sum()\n",
    "    loss.backward(retain_graph=True)\n",
    "    \n",
    "    if not torch.equal(data[0].grad[:, t+1:, :], torch.zeros_like(data[0].grad)[:, t+1:, :]):\n",
    "        print(data[0].grad - torch.zeros_like(data[0].grad))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.3040e-07,  1.7163e-06, -2.3446e-06],\n",
      "         [-3.6704e-07,  2.1040e-06, -2.2839e-06],\n",
      "         [-3.2477e-07,  2.0537e-06, -2.3068e-06],\n",
      "         ...,\n",
      "         [-4.2247e-07,  2.1317e-06, -2.2747e-06],\n",
      "         [-3.3373e-07,  2.1501e-06, -2.3997e-06],\n",
      "         [-4.6078e-07,  2.0635e-06, -1.9624e-06]],\n",
      "\n",
      "        [[ 3.6047e-10,  7.7162e-09, -1.8404e-09],\n",
      "         [-1.2357e-06,  2.7543e-06, -1.2497e-06],\n",
      "         [-3.8933e-09,  3.6174e-09, -6.1311e-09],\n",
      "         ...,\n",
      "         [-9.4018e-07,  2.5413e-06, -1.7906e-06],\n",
      "         [-7.6363e-07,  2.2541e-06, -1.5270e-06],\n",
      "         [-8.0445e-07,  2.3236e-06, -1.5771e-06]],\n",
      "\n",
      "        [[-3.9525e-07,  2.1637e-06, -2.1691e-06],\n",
      "         [-5.8596e-07,  2.1364e-06, -1.7537e-06],\n",
      "         [-1.6422e-07,  1.9196e-06, -2.5078e-06],\n",
      "         ...,\n",
      "         [-3.9296e-09,  4.7845e-09, -6.5392e-09],\n",
      "         [-4.0689e-07,  2.0476e-06, -2.1090e-06],\n",
      "         [-5.0746e-07,  2.1616e-06, -2.0194e-06]],\n",
      "\n",
      "        [[-4.3714e-07,  2.0936e-06, -2.1059e-06],\n",
      "         [-3.3566e-07,  2.0295e-06, -2.2297e-06],\n",
      "         [-4.7799e-07,  2.2786e-06, -2.3424e-06],\n",
      "         ...,\n",
      "         [-3.4258e-07,  2.0281e-06, -2.2068e-06],\n",
      "         [ 8.0375e-09, -1.4330e-08,  1.4784e-08],\n",
      "         [-1.0792e-06,  2.7477e-06, -1.6712e-06]],\n",
      "\n",
      "        [[-9.4830e-07,  2.5296e-06, -1.5996e-06],\n",
      "         [ 1.7833e-08, -1.8442e-08,  2.8674e-08],\n",
      "         [-6.9452e-07,  2.3684e-06, -1.9512e-06],\n",
      "         ...,\n",
      "         [-6.4230e-07,  2.0118e-06, -1.4945e-06],\n",
      "         [-1.6082e-07,  1.8539e-06, -2.3821e-06],\n",
      "         [-5.0176e-07,  2.1977e-06, -2.1071e-06]]])\n",
      "tensor([[[-1.3411e-07,  1.7171e-06, -2.3784e-06],\n",
      "         [-3.4612e-07,  2.0421e-06, -2.2228e-06],\n",
      "         [-3.2733e-07,  2.0485e-06, -2.3005e-06],\n",
      "         ...,\n",
      "         [-3.5694e-07,  1.9805e-06, -2.1022e-06],\n",
      "         [-3.8495e-07,  2.3119e-06, -2.5735e-06],\n",
      "         [-5.1622e-07,  2.1840e-06, -2.0903e-06]],\n",
      "\n",
      "        [[-4.7610e-07,  2.2790e-06, -2.2535e-06],\n",
      "         [-1.2489e-06,  2.7761e-06, -1.2712e-06],\n",
      "         [-3.6106e-07,  2.1315e-06, -2.3137e-06],\n",
      "         ...,\n",
      "         [-8.6285e-07,  2.4111e-06, -1.6582e-06],\n",
      "         [-7.5353e-07,  2.2408e-06, -1.5100e-06],\n",
      "         [-9.0008e-07,  2.5117e-06, -1.7495e-06]],\n",
      "\n",
      "        [[-4.4421e-07,  2.2097e-06, -2.3189e-06],\n",
      "         [-7.2233e-07,  2.4636e-06, -2.0566e-06],\n",
      "         [-1.5244e-07,  1.8588e-06, -2.4284e-06],\n",
      "         ...,\n",
      "         [-3.7206e-07,  2.1203e-06, -2.2729e-06],\n",
      "         [ 1.0854e-08, -1.5988e-08,  1.8940e-08],\n",
      "         [ 1.3608e-09, -2.2814e-09,  2.4585e-09]],\n",
      "\n",
      "        [[-5.1573e-07,  2.3822e-06, -2.3519e-06],\n",
      "         [-3.8353e-07,  2.1287e-06, -2.3455e-06],\n",
      "         [-3.9305e-07,  2.0572e-06, -2.1147e-06],\n",
      "         ...,\n",
      "         [-3.5517e-07,  2.0621e-06, -2.2432e-06],\n",
      "         [-5.4939e-07,  2.1784e-06, -1.9685e-06],\n",
      "         [-1.0214e-06,  2.6356e-06, -1.5748e-06]],\n",
      "\n",
      "        [[-1.0242e-06,  2.7289e-06, -1.7267e-06],\n",
      "         [-5.2881e-07,  2.1135e-06, -1.8675e-06],\n",
      "         [ 4.1662e-09, -6.2672e-09,  7.3040e-09],\n",
      "         ...,\n",
      "         [-6.5895e-07,  2.0458e-06, -1.5267e-06],\n",
      "         [-1.8351e-07,  1.9033e-06, -2.4377e-06],\n",
      "         [ 3.5642e-09, -7.4009e-09,  6.8829e-09]]])\n",
      "tensor([[[ 3.0513e-09,  6.0679e-08, -1.4422e-08],\n",
      "         [-3.5291e-07,  2.0549e-06, -2.2340e-06],\n",
      "         [ 4.6671e-09, -8.0859e-09,  8.5132e-09],\n",
      "         ...,\n",
      "         [-3.4112e-07,  1.9422e-06, -2.0589e-06],\n",
      "         [-3.2460e-07,  2.1292e-06, -2.3779e-06],\n",
      "         [-4.9607e-07,  2.1375e-06, -2.0415e-06]],\n",
      "\n",
      "        [[-4.6109e-07,  2.1972e-06, -2.1787e-06],\n",
      "         [-1.2903e-06,  2.8516e-06, -1.3310e-06],\n",
      "         [-3.5606e-07,  2.1092e-06, -2.2887e-06],\n",
      "         ...,\n",
      "         [-7.8672e-07,  2.2962e-06, -1.5303e-06],\n",
      "         [-9.1718e-07,  2.5374e-06, -1.8010e-06],\n",
      "         [-8.2395e-07,  2.3657e-06, -1.6129e-06]],\n",
      "\n",
      "        [[-4.2700e-07,  2.1015e-06, -2.2169e-06],\n",
      "         [-6.7212e-07,  2.3405e-06, -1.9435e-06],\n",
      "         [-9.9251e-08,  1.7631e-06, -2.3037e-06],\n",
      "         ...,\n",
      "         [-3.9767e-07,  2.1809e-06, -2.3447e-06],\n",
      "         [-4.2124e-07,  2.1043e-06, -2.1684e-06],\n",
      "         [-5.5449e-07,  2.2764e-06, -2.1346e-06]],\n",
      "\n",
      "        [[-4.9931e-07,  2.3260e-06, -2.3066e-06],\n",
      "         [-3.1521e-07,  1.9757e-06, -2.1681e-06],\n",
      "         [-4.2130e-07,  2.1209e-06, -2.1802e-06],\n",
      "         ...,\n",
      "         [-3.6627e-07,  2.0865e-06, -2.2692e-06],\n",
      "         [-5.3709e-07,  2.1461e-06, -1.9376e-06],\n",
      "         [-1.0163e-06,  2.6295e-06, -1.5655e-06]],\n",
      "\n",
      "        [[-9.5210e-07,  2.6487e-06, -1.6349e-06],\n",
      "         [-5.8723e-07,  2.1981e-06, -1.9771e-06],\n",
      "         [-7.1566e-07,  2.4090e-06, -1.9932e-06],\n",
      "         ...,\n",
      "         [-6.9396e-07,  2.1300e-06, -1.5990e-06],\n",
      "         [-1.3642e-07,  1.7745e-06, -2.2874e-06],\n",
      "         [ 3.5436e-09, -7.3582e-09,  6.8432e-09]]])\n",
      "tensor([[[-1.3720e-07,  2.1500e-06, -2.7034e-06],\n",
      "         [-3.5919e-07,  2.0683e-06, -2.2461e-06],\n",
      "         [ 4.5023e-09, -7.8004e-09,  8.2126e-09],\n",
      "         ...,\n",
      "         [-3.6398e-07,  2.0040e-06, -2.1281e-06],\n",
      "         [-3.8865e-07,  2.3245e-06, -2.5871e-06],\n",
      "         [-4.8481e-07,  2.1007e-06, -2.0052e-06]],\n",
      "\n",
      "        [[-4.4654e-07,  2.0386e-06, -2.0764e-06],\n",
      "         [-1.2255e-06,  2.7388e-06, -1.2315e-06],\n",
      "         [-3.9371e-07,  2.1916e-06, -2.3941e-06],\n",
      "         ...,\n",
      "         [-7.9134e-07,  2.3241e-06, -1.5420e-06],\n",
      "         [-8.6232e-07,  2.4462e-06, -1.7050e-06],\n",
      "         [-8.1824e-07,  2.3682e-06, -1.6049e-06]],\n",
      "\n",
      "        [[-3.9364e-07,  1.9723e-06, -2.0625e-06],\n",
      "         [-6.8620e-07,  2.3736e-06, -1.9743e-06],\n",
      "         [-1.5976e-07,  1.9096e-06, -2.4948e-06],\n",
      "         ...,\n",
      "         [-3.7558e-07,  2.1397e-06, -2.2938e-06],\n",
      "         [-4.1550e-07,  2.0912e-06, -2.1539e-06],\n",
      "         [ 1.3923e-09, -2.3342e-09,  2.5155e-09]],\n",
      "\n",
      "        [[-4.7900e-07,  2.2465e-06, -2.2356e-06],\n",
      "         [-3.3982e-07,  2.0289e-06, -2.2299e-06],\n",
      "         [-4.1924e-07,  2.1167e-06, -2.1759e-06],\n",
      "         ...,\n",
      "         [-3.6560e-07,  2.0915e-06, -2.2747e-06],\n",
      "         [-5.8905e-07,  2.2636e-06, -2.0531e-06],\n",
      "         [-9.6325e-07,  2.5125e-06, -1.4799e-06]],\n",
      "\n",
      "        [[-9.3492e-07,  2.4245e-06, -1.5586e-06],\n",
      "         [-5.9389e-07,  2.2314e-06, -2.0050e-06],\n",
      "         [-6.0015e-07,  2.1539e-06, -1.7456e-06],\n",
      "         ...,\n",
      "         [-6.6544e-07,  2.0634e-06, -1.5408e-06],\n",
      "         [ 1.1321e-08, -1.9104e-08,  2.0469e-08],\n",
      "         [-5.2122e-07,  2.2557e-06, -2.1620e-06]]])\n"
     ]
    }
   ],
   "source": [
    "num_layers = 4\n",
    "# batch_size=5, seq_length=6, feature_size=3\n",
    "length = 100\n",
    "data = [torch.rand(5, length, 3).requires_grad_()  for i in range(num_layers)]\n",
    "data2 = data[0]\n",
    "\n",
    "packed_data_list = []\n",
    "for i in data:\n",
    "    temp, batchs, *_ = pack_sequence(i)\n",
    "    packed_data_list.append(temp)\n",
    "\n",
    "    \n",
    "packed_data, data = generate_test_data(packed_data_list, num_layers)\n",
    "# check the interator works properately.\n",
    "class test_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.att = DotAtten_Layer(embed_dim=3, window_size=1)\n",
    "        # self.att = NTMAtten_Layer(embed_dim=3, window_size=1)\n",
    "        self.fc = nn.Linear(3, 3)\n",
    "    def forward(self, query, key, value):\n",
    "        x, watt = self.att(query, key, value)\n",
    "        x = self.fc(x)\n",
    "        return x, watt\n",
    "\n",
    "test_att = test_model()\n",
    "criteria = nn.MSELoss()\n",
    "\n",
    "query = torch.rand(5, 1, 3)\n",
    "outputs_hat = []\n",
    "for frame, t in interatie_packedsequences(packed_data):\n",
    "    for i in range(num_layers):\n",
    "        assert torch.equal(frame[:, i,:],data[i][:, t, :]), \"{}\\n{} \\n==============\\n {}\".format(i,frame[:, i,:],data[i][:, t, :])\n",
    "        \n",
    "    # query = torch.rand(5, 1, 3)\n",
    "    output_hat, weights = test_att(query, frame, frame)\n",
    "    query = output_hat\n",
    "    outputs_hat.append(output_hat.softmax(dim=-1))\n",
    "    \n",
    "    \n",
    "outputs_hat = torch.cat(outputs_hat, dim=1)\n",
    "outputs = torch.ones(5, length, 3)\n",
    "\n",
    "loss = criteria(outputs_hat, outputs)\n",
    "loss.backward()\n",
    "    \n",
    "for i in data:\n",
    "    print(i.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 100, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
