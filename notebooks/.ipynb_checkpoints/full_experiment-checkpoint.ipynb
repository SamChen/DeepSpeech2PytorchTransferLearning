{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic layers\n",
    "1. Brelu which is relu with a cutoff. In paddlepaddle0.10, the defaut threshould is 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BReLU(nn.Hardtanh):\n",
    "    r\"\"\"Applies the element-wise function:\n",
    "\n",
    "    .. math::\n",
    "        \\text{ReLU6}(x) = \\min(\\max(0,x), cutoff)\n",
    "\n",
    "    Args:\n",
    "        inplace: can optionally do the operation in-place. Default: ``False``\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *)` where `*` means, any number of additional\n",
    "          dimensions\n",
    "        - Output: :math:`(N, *)`, same shape as the input\n",
    "\n",
    "    .. image:: scripts/activation_images/ReLU6.png\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = nn.ReLU6()\n",
    "        >>> input = torch.randn(2)\n",
    "        >>> output = m(input)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cutoff=24., inplace=False):\n",
    "        super(BReLU, self).__init__(0., cutoff, inplace)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        inplace_str = 'inplace=True' if self.inplace else ''\n",
    "        return inplace_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mask(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mask, self).__init__()\n",
    "        \n",
    "    def forward(self,x, length):\n",
    "        mask = torch.zeros_like(x, dtype=torch.float32)\n",
    "        for index, length in enumerate(length):\n",
    "            mask[index, :, :length, :] = 1\n",
    "        return x * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(5, 10)\n",
    "b = torch.ones(5, 10)\n",
    "c = torch.ones(4, 10)\n",
    "d = torch.ones(4, 10)\n",
    "sequences = nn.utils.rnn.pad_sequence([a, b, c, d], padding_value=0.4, batch_first=True)\n",
    "sequences = sequences.unsqueeze(1) # change it into image format\n",
    "sequence_lengths = torch.Tensor([5,5 ,4,4]).type(torch.LongTensor)\n",
    "mask = Mask()\n",
    "masked_seq = mask(sequences, sequence_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv+bn+mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_bn_mask(nn.Module):\n",
    "    def __init__(self, ichannel, ochannel, kernel_size, padding, stride, bias=False, track_running_stats=False):\n",
    "        super(conv_bn_mask, self).__init__()\n",
    "        self.conv = nn.Conv2d(ichannel, ochannel, kernel_size=kernel_size, padding=padding, stride=stride, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(ochannel, track_running_stats=track_running_stats)\n",
    "        self.activation = BReLU(cutoff=24)\n",
    "        self.mask = Mask()\n",
    "    def forward(self, x, length):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.mask(x, length)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image flatten and sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatten\n",
    "image.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 5, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = torch.ones(5, 10)\n",
    "# b = torch.ones(4, 10)\n",
    "# c = torch.ones(4, 10)\n",
    "# sequences = nn.utils.rnn.pad_sequence([a, b, c], padding_value=0.4, batch_first=True)\n",
    "\n",
    "flattened_seq = masked_seq.view(4, -1, 10)\n",
    "flattened_seq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sequence \n",
    "nn.utils.rnn.pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]), batch_sizes=tensor([4, 4, 4, 4, 2]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = nn.utils.rnn.pack_padded_sequence(flattened_seq, sequence_lengths, batch_first=True)\n",
    "seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bidirectional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_gru import BidirRNNLayer, GRUCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigru = BidirRNNLayer(GRUCell, input_size=10, hidden_size=20, gate_act=\"relu\", state_act=\"tanh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigru_result, _ = bigru.forward(seqs, torch.zeros((2, 4, 20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigru_result_reshaped = nn.utils.rnn.pad_packed_sequence(bigru_result, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 4, 4, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigru_result.batch_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### last fc \n",
    "2048 * 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_data, batch_sizes, _, _ = bigru_result\n",
    "bottleneck = nn.Linear(40, 28)\n",
    "bottleneck_result = bottleneck(bottleneck_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output layer\n",
    "softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.LogSoftmax(dim=0)\n",
    "prob = softmax(bottleneck_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([[-4.1676e+09, -5.0515e+07, -4.4991e+10, -1.5702e+10, -6.9315e-01,\n",
      "         -6.6085e+09, -7.7123e+07, -2.4533e+10, -5.3310e+10, -2.1041e+10,\n",
      "         -1.4993e+08, -6.9315e-01, -1.2671e+08, -2.8169e+10, -6.9315e-01,\n",
      "         -1.0366e+08, -6.9315e-01, -6.9315e-01, -6.9315e-01, -2.5552e+10,\n",
      "         -4.9122e+07, -6.9315e-01, -1.3816e+10, -4.5174e+10, -6.9315e-01,\n",
      "         -1.4428e+10, -6.9315e-01, -1.6474e+10],\n",
      "        [-4.1676e+09, -5.0515e+07, -4.4991e+10, -1.5702e+10, -6.9315e-01,\n",
      "         -6.6085e+09, -7.7123e+07, -2.4533e+10, -5.3310e+10, -2.1041e+10,\n",
      "         -1.4993e+08, -6.9315e-01, -1.2671e+08, -2.8169e+10, -6.9315e-01,\n",
      "         -1.0366e+08, -6.9315e-01, -6.9315e-01, -6.9315e-01, -2.5552e+10,\n",
      "         -4.9122e+07, -6.9315e-01, -1.3816e+10, -4.5174e+10, -6.9315e-01,\n",
      "         -1.4428e+10, -6.9315e-01, -1.6474e+10],\n",
      "        [-4.0711e+09, -8.2979e-01, -4.5112e+10, -1.5653e+10, -6.2133e+07,\n",
      "         -6.4808e+09, -1.7119e+00, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "         -2.5039e+04, -3.2592e+07, -1.5936e+04, -2.8167e+10, -2.1838e+08,\n",
      "         -1.6674e+04, -1.0871e+08, -2.6650e+07, -3.8509e+07, -2.5613e+10,\n",
      "         -6.5915e+03, -7.1014e+07, -1.3707e+10, -4.5153e+10, -1.7368e+06,\n",
      "         -1.4462e+10, -1.6372e+08, -1.6550e+10],\n",
      "        [-4.0711e+09, -8.2979e-01, -4.5112e+10, -1.5653e+10, -6.2133e+07,\n",
      "         -6.4808e+09, -1.7119e+00, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "         -2.5039e+04, -3.2592e+07, -1.5936e+04, -2.8167e+10, -2.1838e+08,\n",
      "         -1.6674e+04, -1.0871e+08, -2.6650e+07, -3.8509e+07, -2.5613e+10,\n",
      "         -6.5915e+03, -7.1014e+07, -1.3707e+10, -4.5153e+10, -1.7368e+06,\n",
      "         -1.4462e+10, -1.6372e+08, -1.6550e+10],\n",
      "        [-4.0711e+09, -2.7511e+00, -4.5112e+10, -1.5653e+10, -6.2133e+07,\n",
      "         -6.4808e+09, -1.1411e+00, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "         -2.5039e+04, -3.2592e+07, -1.5936e+04, -2.8167e+10, -2.1838e+08,\n",
      "         -1.6675e+04, -1.0871e+08, -2.6650e+07, -3.8509e+07, -2.5613e+10,\n",
      "         -6.5929e+03, -7.1014e+07, -1.3707e+10, -4.5153e+10, -1.7368e+06,\n",
      "         -1.4462e+10, -1.6372e+08, -1.6550e+10],\n",
      "        [-4.0711e+09, -2.7511e+00, -4.5112e+10, -1.5653e+10, -6.2133e+07,\n",
      "         -6.4808e+09, -1.1411e+00, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "         -2.5039e+04, -3.2592e+07, -1.5936e+04, -2.8167e+10, -2.1838e+08,\n",
      "         -1.6675e+04, -1.0871e+08, -2.6650e+07, -3.8509e+07, -2.5613e+10,\n",
      "         -6.5929e+03, -7.1014e+07, -1.3707e+10, -4.5153e+10, -1.7368e+06,\n",
      "         -1.4462e+10, -1.6372e+08, -1.6550e+10],\n",
      "        [-4.0711e+09, -1.4585e+03, -4.5112e+10, -1.5653e+10, -6.2136e+07,\n",
      "         -6.4808e+09, -2.9474e+02, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "         -2.5131e+04, -3.2592e+07, -1.3124e+04, -2.8167e+10, -2.1838e+08,\n",
      "         -1.5556e+04, -1.0871e+08, -2.6651e+07, -3.8510e+07, -2.5613e+10,\n",
      "         -8.2588e+03, -7.1015e+07, -1.3707e+10, -4.5153e+10, -1.7370e+06,\n",
      "         -1.4462e+10, -1.6372e+08, -1.6550e+10],\n",
      "        [-4.0711e+09, -1.4585e+03, -4.5112e+10, -1.5653e+10, -6.2136e+07,\n",
      "         -6.4808e+09, -2.9474e+02, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "         -2.5131e+04, -3.2592e+07, -1.3124e+04, -2.8167e+10, -2.1838e+08,\n",
      "         -1.5556e+04, -1.0871e+08, -2.6651e+07, -3.8510e+07, -2.5613e+10,\n",
      "         -8.2588e+03, -7.1015e+07, -1.3707e+10, -4.5153e+10, -1.7370e+06,\n",
      "         -1.4462e+10, -1.6372e+08, -1.6550e+10],\n",
      "        [-4.0711e+09, -1.4671e+03, -4.5112e+10, -1.5653e+10, -6.2136e+07,\n",
      "         -6.4808e+09, -2.9019e+02, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "         -2.5147e+04, -3.2592e+07, -1.3126e+04, -2.8167e+10, -2.1838e+08,\n",
      "         -1.5587e+04, -1.0871e+08, -2.6651e+07, -3.8510e+07, -2.5613e+10,\n",
      "         -8.2548e+03, -7.1015e+07, -1.3707e+10, -4.5153e+10, -1.7369e+06,\n",
      "         -1.4462e+10, -1.6372e+08, -1.6550e+10],\n",
      "        [-4.0711e+09, -1.4671e+03, -4.5112e+10, -1.5653e+10, -6.2136e+07,\n",
      "         -6.4808e+09, -2.9019e+02, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "         -2.5147e+04, -3.2592e+07, -1.3126e+04, -2.8167e+10, -2.1838e+08,\n",
      "         -1.5587e+04, -1.0871e+08, -2.6651e+07, -3.8510e+07, -2.5613e+10,\n",
      "         -8.2548e+03, -7.1015e+07, -1.3707e+10, -4.5153e+10, -1.7369e+06,\n",
      "         -1.4462e+10, -1.6372e+08, -1.6550e+10],\n",
      "        [-4.0711e+09, -1.4720e+03, -4.5112e+10, -1.5653e+10, -6.2136e+07,\n",
      "         -6.4808e+09, -2.9069e+02, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "         -2.5149e+04, -3.2592e+07, -1.3125e+04, -2.8167e+10, -2.1838e+08,\n",
      "         -1.5583e+04, -1.0871e+08, -2.6651e+07, -3.8510e+07, -2.5613e+10,\n",
      "         -8.2580e+03, -7.1015e+07, -1.3707e+10, -4.5153e+10, -1.7369e+06,\n",
      "         -1.4462e+10, -1.6372e+08, -1.6550e+10],\n",
      "        [-4.0711e+09, -1.4720e+03, -4.5112e+10, -1.5653e+10, -6.2136e+07,\n",
      "         -6.4808e+09, -2.9069e+02, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "         -2.5149e+04, -3.2592e+07, -1.3125e+04, -2.8167e+10, -2.1838e+08,\n",
      "         -1.5583e+04, -1.0871e+08, -2.6651e+07, -3.8510e+07, -2.5613e+10,\n",
      "         -8.2580e+03, -7.1015e+07, -1.3707e+10, -4.5153e+10, -1.7369e+06,\n",
      "         -1.4462e+10, -1.6372e+08, -1.6550e+10],\n",
      "        [-4.0710e+09, -5.1403e+04, -4.5112e+10, -1.5653e+10, -6.2116e+07,\n",
      "         -6.4808e+09, -1.9178e+04, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "         -1.3313e+00, -3.2562e+07, -1.4221e+00, -2.8167e+10, -2.1843e+08,\n",
      "         -1.4399e+00, -1.0869e+08, -2.6592e+07, -3.8457e+07, -2.5613e+10,\n",
      "         -1.4231e+00, -7.0995e+07, -1.3707e+10, -4.5154e+10, -1.7670e+06,\n",
      "         -1.4462e+10, -1.6373e+08, -1.6550e+10],\n",
      "        [-4.0710e+09, -5.1403e+04, -4.5112e+10, -1.5653e+10, -6.2116e+07,\n",
      "         -6.4808e+09, -1.9178e+04, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "         -1.3313e+00, -3.2562e+07, -1.4221e+00, -2.8167e+10, -2.1843e+08,\n",
      "         -1.4399e+00, -1.0869e+08, -2.6592e+07, -3.8457e+07, -2.5613e+10,\n",
      "         -1.4231e+00, -7.0995e+07, -1.3707e+10, -4.5154e+10, -1.7670e+06,\n",
      "         -1.4462e+10, -1.6373e+08, -1.6550e+10],\n",
      "        [-4.0710e+09, -5.1403e+04, -4.5112e+10, -1.5653e+10, -6.2116e+07,\n",
      "         -6.4808e+09, -1.9178e+04, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "         -1.4445e+00, -3.2562e+07, -1.3518e+00, -2.8167e+10, -2.1843e+08,\n",
      "         -1.3354e+00, -1.0869e+08, -2.6592e+07, -3.8457e+07, -2.5613e+10,\n",
      "         -1.3508e+00, -7.0995e+07, -1.3707e+10, -4.5154e+10, -1.7670e+06,\n",
      "         -1.4462e+10, -1.6373e+08, -1.6550e+10],\n",
      "        [-4.0710e+09, -5.1403e+04, -4.5112e+10, -1.5653e+10, -6.2116e+07,\n",
      "         -6.4808e+09, -1.9178e+04, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "         -1.4445e+00, -3.2562e+07, -1.3518e+00, -2.8167e+10, -2.1843e+08,\n",
      "         -1.3354e+00, -1.0869e+08, -2.6592e+07, -3.8457e+07, -2.5613e+10,\n",
      "         -1.3508e+00, -7.0995e+07, -1.3707e+10, -4.5154e+10, -1.7670e+06,\n",
      "         -1.4462e+10, -1.6373e+08, -1.6550e+10],\n",
      "        [-6.9315e-01, -1.7506e+10, -6.9315e-01, -6.9315e-01, -2.2954e+10,\n",
      "         -6.9315e-01, -1.0022e+09, -6.9315e-01, -6.9315e-01, -6.9315e-01,\n",
      "         -1.1361e+10, -2.0480e+10, -3.1774e+10, -6.9315e-01, -5.9908e+09,\n",
      "         -4.1481e+10, -2.2517e+10, -2.8269e+10, -3.7025e+10, -6.9315e-01,\n",
      "         -1.9760e+10, -4.6824e+10, -6.9315e-01, -6.9315e-01, -2.9855e+10,\n",
      "         -6.9315e-01, -3.9037e+10, -6.9315e-01],\n",
      "        [-6.9315e-01, -1.7506e+10, -6.9315e-01, -6.9315e-01, -2.2954e+10,\n",
      "         -6.9315e-01, -1.0022e+09, -6.9315e-01, -6.9315e-01, -6.9315e-01,\n",
      "         -1.1361e+10, -2.0480e+10, -3.1774e+10, -6.9315e-01, -5.9908e+09,\n",
      "         -4.1481e+10, -2.2517e+10, -2.8269e+10, -3.7025e+10, -6.9315e-01,\n",
      "         -1.9760e+10, -4.6824e+10, -6.9315e-01, -6.9315e-01, -2.9855e+10,\n",
      "         -6.9315e-01, -3.9037e+10, -6.9315e-01]], grad_fn=<LogSoftmaxBackward>), batch_sizes=tensor([4, 4, 4, 4, 2]), sorted_indices=None, unsorted_indices=None)\n",
      "(tensor([[[-4.1676e+09, -5.0515e+07, -4.4991e+10, -1.5702e+10, -6.9315e-01,\n",
      "          -6.6085e+09, -7.7123e+07, -2.4533e+10, -5.3310e+10, -2.1041e+10,\n",
      "          -1.4993e+08, -6.9315e-01, -1.2671e+08, -2.8169e+10, -6.9315e-01,\n",
      "          -1.0366e+08, -6.9315e-01, -6.9315e-01, -6.9315e-01, -2.5552e+10,\n",
      "          -4.9122e+07, -6.9315e-01, -1.3816e+10, -4.5174e+10, -6.9315e-01,\n",
      "          -1.4428e+10, -6.9315e-01, -1.6474e+10],\n",
      "         [-4.1676e+09, -5.0515e+07, -4.4991e+10, -1.5702e+10, -6.9315e-01,\n",
      "          -6.6085e+09, -7.7123e+07, -2.4533e+10, -5.3310e+10, -2.1041e+10,\n",
      "          -1.4993e+08, -6.9315e-01, -1.2671e+08, -2.8169e+10, -6.9315e-01,\n",
      "          -1.0366e+08, -6.9315e-01, -6.9315e-01, -6.9315e-01, -2.5552e+10,\n",
      "          -4.9122e+07, -6.9315e-01, -1.3816e+10, -4.5174e+10, -6.9315e-01,\n",
      "          -1.4428e+10, -6.9315e-01, -1.6474e+10],\n",
      "         [-4.0711e+09, -8.2979e-01, -4.5112e+10, -1.5653e+10, -6.2133e+07,\n",
      "          -6.4808e+09, -1.7119e+00, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "          -2.5039e+04, -3.2592e+07, -1.5936e+04, -2.8167e+10, -2.1838e+08,\n",
      "          -1.6674e+04, -1.0871e+08, -2.6650e+07, -3.8509e+07, -2.5613e+10,\n",
      "          -6.5915e+03, -7.1014e+07, -1.3707e+10, -4.5153e+10, -1.7368e+06,\n",
      "          -1.4462e+10, -1.6372e+08, -1.6550e+10],\n",
      "         [-4.0711e+09, -8.2979e-01, -4.5112e+10, -1.5653e+10, -6.2133e+07,\n",
      "          -6.4808e+09, -1.7119e+00, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "          -2.5039e+04, -3.2592e+07, -1.5936e+04, -2.8167e+10, -2.1838e+08,\n",
      "          -1.6674e+04, -1.0871e+08, -2.6650e+07, -3.8509e+07, -2.5613e+10,\n",
      "          -6.5915e+03, -7.1014e+07, -1.3707e+10, -4.5153e+10, -1.7368e+06,\n",
      "          -1.4462e+10, -1.6372e+08, -1.6550e+10]],\n",
      "\n",
      "        [[-4.0711e+09, -2.7511e+00, -4.5112e+10, -1.5653e+10, -6.2133e+07,\n",
      "          -6.4808e+09, -1.1411e+00, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "          -2.5039e+04, -3.2592e+07, -1.5936e+04, -2.8167e+10, -2.1838e+08,\n",
      "          -1.6675e+04, -1.0871e+08, -2.6650e+07, -3.8509e+07, -2.5613e+10,\n",
      "          -6.5929e+03, -7.1014e+07, -1.3707e+10, -4.5153e+10, -1.7368e+06,\n",
      "          -1.4462e+10, -1.6372e+08, -1.6550e+10],\n",
      "         [-4.0711e+09, -2.7511e+00, -4.5112e+10, -1.5653e+10, -6.2133e+07,\n",
      "          -6.4808e+09, -1.1411e+00, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "          -2.5039e+04, -3.2592e+07, -1.5936e+04, -2.8167e+10, -2.1838e+08,\n",
      "          -1.6675e+04, -1.0871e+08, -2.6650e+07, -3.8509e+07, -2.5613e+10,\n",
      "          -6.5929e+03, -7.1014e+07, -1.3707e+10, -4.5153e+10, -1.7368e+06,\n",
      "          -1.4462e+10, -1.6372e+08, -1.6550e+10],\n",
      "         [-4.0711e+09, -1.4585e+03, -4.5112e+10, -1.5653e+10, -6.2136e+07,\n",
      "          -6.4808e+09, -2.9474e+02, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "          -2.5131e+04, -3.2592e+07, -1.3124e+04, -2.8167e+10, -2.1838e+08,\n",
      "          -1.5556e+04, -1.0871e+08, -2.6651e+07, -3.8510e+07, -2.5613e+10,\n",
      "          -8.2588e+03, -7.1015e+07, -1.3707e+10, -4.5153e+10, -1.7370e+06,\n",
      "          -1.4462e+10, -1.6372e+08, -1.6550e+10],\n",
      "         [-4.0711e+09, -1.4585e+03, -4.5112e+10, -1.5653e+10, -6.2136e+07,\n",
      "          -6.4808e+09, -2.9474e+02, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "          -2.5131e+04, -3.2592e+07, -1.3124e+04, -2.8167e+10, -2.1838e+08,\n",
      "          -1.5556e+04, -1.0871e+08, -2.6651e+07, -3.8510e+07, -2.5613e+10,\n",
      "          -8.2588e+03, -7.1015e+07, -1.3707e+10, -4.5153e+10, -1.7370e+06,\n",
      "          -1.4462e+10, -1.6372e+08, -1.6550e+10]],\n",
      "\n",
      "        [[-4.0711e+09, -1.4671e+03, -4.5112e+10, -1.5653e+10, -6.2136e+07,\n",
      "          -6.4808e+09, -2.9019e+02, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "          -2.5147e+04, -3.2592e+07, -1.3126e+04, -2.8167e+10, -2.1838e+08,\n",
      "          -1.5587e+04, -1.0871e+08, -2.6651e+07, -3.8510e+07, -2.5613e+10,\n",
      "          -8.2548e+03, -7.1015e+07, -1.3707e+10, -4.5153e+10, -1.7369e+06,\n",
      "          -1.4462e+10, -1.6372e+08, -1.6550e+10],\n",
      "         [-4.0711e+09, -1.4671e+03, -4.5112e+10, -1.5653e+10, -6.2136e+07,\n",
      "          -6.4808e+09, -2.9019e+02, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "          -2.5147e+04, -3.2592e+07, -1.3126e+04, -2.8167e+10, -2.1838e+08,\n",
      "          -1.5587e+04, -1.0871e+08, -2.6651e+07, -3.8510e+07, -2.5613e+10,\n",
      "          -8.2548e+03, -7.1015e+07, -1.3707e+10, -4.5153e+10, -1.7369e+06,\n",
      "          -1.4462e+10, -1.6372e+08, -1.6550e+10],\n",
      "         [-4.0711e+09, -1.4720e+03, -4.5112e+10, -1.5653e+10, -6.2136e+07,\n",
      "          -6.4808e+09, -2.9069e+02, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "          -2.5149e+04, -3.2592e+07, -1.3125e+04, -2.8167e+10, -2.1838e+08,\n",
      "          -1.5583e+04, -1.0871e+08, -2.6651e+07, -3.8510e+07, -2.5613e+10,\n",
      "          -8.2580e+03, -7.1015e+07, -1.3707e+10, -4.5153e+10, -1.7369e+06,\n",
      "          -1.4462e+10, -1.6372e+08, -1.6550e+10],\n",
      "         [-4.0711e+09, -1.4720e+03, -4.5112e+10, -1.5653e+10, -6.2136e+07,\n",
      "          -6.4808e+09, -2.9069e+02, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "          -2.5149e+04, -3.2592e+07, -1.3125e+04, -2.8167e+10, -2.1838e+08,\n",
      "          -1.5583e+04, -1.0871e+08, -2.6651e+07, -3.8510e+07, -2.5613e+10,\n",
      "          -8.2580e+03, -7.1015e+07, -1.3707e+10, -4.5153e+10, -1.7369e+06,\n",
      "          -1.4462e+10, -1.6372e+08, -1.6550e+10]],\n",
      "\n",
      "        [[-4.0710e+09, -5.1403e+04, -4.5112e+10, -1.5653e+10, -6.2116e+07,\n",
      "          -6.4808e+09, -1.9178e+04, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "          -1.3313e+00, -3.2562e+07, -1.4221e+00, -2.8167e+10, -2.1843e+08,\n",
      "          -1.4399e+00, -1.0869e+08, -2.6592e+07, -3.8457e+07, -2.5613e+10,\n",
      "          -1.4231e+00, -7.0995e+07, -1.3707e+10, -4.5154e+10, -1.7670e+06,\n",
      "          -1.4462e+10, -1.6373e+08, -1.6550e+10],\n",
      "         [-4.0710e+09, -5.1403e+04, -4.5112e+10, -1.5653e+10, -6.2116e+07,\n",
      "          -6.4808e+09, -1.9178e+04, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "          -1.3313e+00, -3.2562e+07, -1.4221e+00, -2.8167e+10, -2.1843e+08,\n",
      "          -1.4399e+00, -1.0869e+08, -2.6592e+07, -3.8457e+07, -2.5613e+10,\n",
      "          -1.4231e+00, -7.0995e+07, -1.3707e+10, -4.5154e+10, -1.7670e+06,\n",
      "          -1.4462e+10, -1.6373e+08, -1.6550e+10],\n",
      "         [-4.0710e+09, -5.1403e+04, -4.5112e+10, -1.5653e+10, -6.2116e+07,\n",
      "          -6.4808e+09, -1.9178e+04, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "          -1.4445e+00, -3.2562e+07, -1.3518e+00, -2.8167e+10, -2.1843e+08,\n",
      "          -1.3354e+00, -1.0869e+08, -2.6592e+07, -3.8457e+07, -2.5613e+10,\n",
      "          -1.3508e+00, -7.0995e+07, -1.3707e+10, -4.5154e+10, -1.7670e+06,\n",
      "          -1.4462e+10, -1.6373e+08, -1.6550e+10],\n",
      "         [-4.0710e+09, -5.1403e+04, -4.5112e+10, -1.5653e+10, -6.2116e+07,\n",
      "          -6.4808e+09, -1.9178e+04, -2.4576e+10, -5.2996e+10, -2.1001e+10,\n",
      "          -1.4445e+00, -3.2562e+07, -1.3518e+00, -2.8167e+10, -2.1843e+08,\n",
      "          -1.3354e+00, -1.0869e+08, -2.6592e+07, -3.8457e+07, -2.5613e+10,\n",
      "          -1.3508e+00, -7.0995e+07, -1.3707e+10, -4.5154e+10, -1.7670e+06,\n",
      "          -1.4462e+10, -1.6373e+08, -1.6550e+10]],\n",
      "\n",
      "        [[-6.9315e-01, -1.7506e+10, -6.9315e-01, -6.9315e-01, -2.2954e+10,\n",
      "          -6.9315e-01, -1.0022e+09, -6.9315e-01, -6.9315e-01, -6.9315e-01,\n",
      "          -1.1361e+10, -2.0480e+10, -3.1774e+10, -6.9315e-01, -5.9908e+09,\n",
      "          -4.1481e+10, -2.2517e+10, -2.8269e+10, -3.7025e+10, -6.9315e-01,\n",
      "          -1.9760e+10, -4.6824e+10, -6.9315e-01, -6.9315e-01, -2.9855e+10,\n",
      "          -6.9315e-01, -3.9037e+10, -6.9315e-01],\n",
      "         [-6.9315e-01, -1.7506e+10, -6.9315e-01, -6.9315e-01, -2.2954e+10,\n",
      "          -6.9315e-01, -1.0022e+09, -6.9315e-01, -6.9315e-01, -6.9315e-01,\n",
      "          -1.1361e+10, -2.0480e+10, -3.1774e+10, -6.9315e-01, -5.9908e+09,\n",
      "          -4.1481e+10, -2.2517e+10, -2.8269e+10, -3.7025e+10, -6.9315e-01,\n",
      "          -1.9760e+10, -4.6824e+10, -6.9315e-01, -6.9315e-01, -2.9855e+10,\n",
      "          -6.9315e-01, -3.9037e+10, -6.9315e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00]]], grad_fn=<CopySlices>), tensor([5, 5, 4, 4]))\n"
     ]
    }
   ],
   "source": [
    "output = nn.utils.rnn.PackedSequence(prob, batch_sizes)\n",
    "print(output)\n",
    "output = nn.utils.rnn.pad_packed_sequence(output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelyhood, seq_lengths = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTC loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengths = seq_lengths\n",
    "target_lengths = seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.randint(low=1, high=log_likelyhood.shape[2], size=(4, 5), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_loss = nn.CTCLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ctc_loss(log_likelyhood, target, input_lengths, target_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
