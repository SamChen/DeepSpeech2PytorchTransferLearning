{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.append(\"../model_utils/\")\n",
    "import custom_gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_model(nn.Module):\n",
    "    def __init__(self, sub_model, input_size, output_size):\n",
    "        super(test_model, self).__init__()\n",
    "        self.criteria = None\n",
    "        self.sub_model = sub_model\n",
    "        self.fc = nn.Linear(input_size=input_size, output_size=output_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.sub_model(x, hidden)\n",
    "        x = self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pt = nn.LSTM(input_size=3, hidden_size=5, batch_first=True)\n",
    "lstm_cell_pt = nn.LSTMCell(input_size=3, hidden_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "\n",
    "def lstm_initialization(lstm_model, seed = 9):\n",
    "    model = copy.deepcopy(lstm_model)\n",
    "    torch.manual_seed(seed)\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' in name:\n",
    "            nn.init.constant_(param, 0.3)\n",
    "        elif 'weight' in name:\n",
    "            nn.init.xavier_uniform_(param)\n",
    "    return model\n",
    "    \n",
    "\n",
    "lstm_cell_pt = lstm_initialization(lstm_cell_pt) \n",
    "lstm_pt = lstm_initialization(lstm_pt) \n",
    "# test 1. make sure weights are initialized the same\n",
    "def parameter_checking(model_a, model_b):\n",
    "    for param_pair in zip(model_a.parameters(), model_b.parameters()):\n",
    "        assert torch.all(torch.eq(*param_pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.zeros([1,3,5]).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 2 make sure outputs are the same\n",
    "\n",
    "input = torch.rand(3, 1, 3)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "initial_hidden_pt = [torch.zeros([1, 3, 5]).requires_grad_(), torch.zeros([1, 3, 5]).requires_grad_()]\n",
    "initial_hidden_cell_pt = [torch.zeros([3, 5]).requires_grad_() for _ in initial_hidden_pt]\n",
    "\n",
    "lstm_pt = nn.LSTM(input_size=3, hidden_size=5, batch_first=True)\n",
    "lstm_cell_pt = nn.LSTMCell(input_size=3, hidden_size=5)\n",
    "\n",
    "lstm_pt = lstm_initialization(lstm_pt) \n",
    "lstm_cell_pt = lstm_initialization(lstm_cell_pt) \n",
    "\n",
    "\n",
    "output_pt = lstm_pt(input, initial_hidden_pt)\n",
    "output_cell_pt = lstm_cell_pt(torch.squeeze(input, 1) , initial_hidden_cell_pt)\n",
    "\n",
    "assert torch.all(torch.eq(output_cell_pt[0], output_pt[1][0]))\n",
    "\n",
    "# make sure the gradient are the same\n",
    "\n",
    "hyp_cell_pt = output_cell_pt[0]\n",
    "hyp_pt = output_pt[0]\n",
    "hyp_pt = torch.squeeze(hyp_pt, 1)\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def get_gradient(model, criteria, hyp, ref):\n",
    "    loss = criteria(hyp, ref)\n",
    "    loss.backward()\n",
    "    \n",
    "    \n",
    "get_gradient(lstm_cell_pt, criteria, hyp_cell_pt, ref=target)\n",
    "get_gradient(lstm_pt, criteria, hyp_pt, ref=target)\n",
    "for param_pair in zip(lstm_cell_pt.parameters(), lstm_pt.parameters()):\n",
    "    assert torch.all(torch.eq(param_pair[0].grad, param_pair[1].grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]], requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0112,  0.0251,  0.0491,  0.0502,  0.0306],\n",
       "         [ 0.0422, -0.0152, -0.0086, -0.0186,  0.0391],\n",
       "         [ 0.0008, -0.0027,  0.0045, -0.0043, -0.0243]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_hidden_pt[0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]], requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_hidden_cell_pt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0112,  0.0251,  0.0491,  0.0502,  0.0306],\n",
       "        [ 0.0422, -0.0152, -0.0086, -0.0186,  0.0391],\n",
       "        [ 0.0008, -0.0027,  0.0045, -0.0043, -0.0243]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_hidden_cell_pt[0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell_format(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMCell_format, self).__init__()\n",
    "        self.lstm = nn.LSTMCell(input_size=input_size, hidden_size=hidden_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        h, c = self.lstm(input, hidden)\n",
    "        return h, [h,c]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 10000\n",
    "input = torch.rand(3, length, 3)\n",
    "target = torch.empty([3,length], dtype=torch.long).random_(5)\n",
    "initial_hidden = [torch.zeros([1, 3, 5]), torch.zeros([1, 3, 5])]\n",
    "\n",
    "lstm_custom = custom_gru.RNNLayer(LSTMCell_format, input_size=3, hidden_size=5)\n",
    "lstm_custom = lstm_initialization(lstm_custom)\n",
    "output_custom = lstm_custom(nn.utils.rnn.pack_padded_sequence(input, [length, length, length], batch_first=True), [torch.squeeze(i, 0) for i in initial_hidden])\n",
    "\n",
    "lstm_pt = nn.LSTM(input_size=3, hidden_size=5, batch_first=True)\n",
    "lstm_pt = lstm_initialization(lstm_pt) \n",
    "output_pt = lstm_pt(input, initial_hidden)\n",
    "\n",
    "# lstm_cell_pt = nn.LSTMCell(input_size=3, hidden_size=5)\n",
    "# lstm_cell_pt = lstm_initialization(lstm_cell_pt) \n",
    "# output_cell_pt = lstm_cell_pt(torch.squeeze(input, 1) , [torch.squeeze(i, 0) for i in initial_hidden])\n",
    "\n",
    "\n",
    "assert torch.all(torch.eq(output_custom[1][1], output_pt[1][1]))\n",
    "output_custom = nn.utils.rnn.pad_packed_sequence(output_custom[0], batch_first=True)[0]\n",
    "assert torch.all(torch.eq(output_custom, output_pt[0]))\n",
    "\n",
    "hyp_custom = torch.transpose(output_custom, 2, 1)\n",
    "\n",
    "hyp_pt = output_pt[0]\n",
    "hyp_pt = torch.transpose(hyp_pt, 2, 1)\n",
    "\n",
    "# hyp_cell_pt = torch.unsqueeze(output_cell_pt[0], 2)\n",
    "# assert torch.all(torch.eq(hyp_cell_pt, hyp_pt)), \"{}, {}\".format(hyp_cell_pt.shape, hyp_pt.shape)\n",
    "\n",
    "criteria = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "    \n",
    "get_gradient(lstm_pt, criteria, hyp_pt, ref=target)\n",
    "# get_gradient(lstm_cell_pt, criteria, hyp_cell_pt, ref=target)\n",
    "get_gradient(lstm_custom, criteria, hyp_custom, ref=target)\n",
    "\n",
    "# for param_pair in zip(lstm_cell_pt.parameters(), lstm_pt.parameters()):\n",
    "#     assert torch.all(torch.eq(param_pair[0].grad, param_pair[1].grad)), \"{}, {}\".format(param_pair[0].grad, param_pair[1].grad)\n",
    "    \n",
    "for param_pair in zip(lstm_custom.parameters(), lstm_pt.parameters()):\n",
    "    assert torch.allclose(param_pair[0].grad, param_pair[1].grad, atol=1e-08), \"{}\".format(param_pair[0].grad - param_pair[1].grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "tensor(9.4702e-09)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "tensor(0.)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "tensor(1.0060e-08)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "for param_pair in zip(lstm_custom.parameters(), lstm_pt.parameters()):\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(torch.std(param_pair[0].grad - param_pair[1].grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pt[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
